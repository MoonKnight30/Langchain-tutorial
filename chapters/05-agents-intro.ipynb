{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzhY6yaJvra"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/05-agents-intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdh8R2hHyBe"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGyuCmfjHyBf"
      },
      "source": [
        "# LangChain Agents Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6pVPE5wHyBf"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Agents, adding the ability to use tools such as search and calculators to complete tasks that normal LLMs cannot fufil. In this example we will be using OpenAI's `gpt-4o-mini`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7fUwugBtH00s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b74612-3217-4a37-9a1f-a5627bfe3f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/441.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m358.4/441.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core \\\n",
        "  langchain-google-genai \\\n",
        "  langchain-community \\\n",
        "  langsmith \\\n",
        "  google-generativeai \\\n",
        "  google-search-results==2.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj6kiBZ-HyBg"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz64zYRKHyBg"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1E7bzVbHyBg",
        "outputId": "20fba6c8-03d4-4ddf-db6e-78b8f4238bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n",
            "Enter Google API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# LangSmith Setup (optional, for observability)\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-prompts-gemini\"\n",
        "\n",
        "# Google Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or \\\n",
        "    getpass(\"Enter Google API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVozCSrXHyBg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgbH7TfgHyBg"
      },
      "source": [
        "## Introduction to Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZp6eo8RHyBg"
      },
      "source": [
        "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it. Let's start by creating a few simple tools.\n",
        "\n",
        "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:\n",
        "\n",
        "* A docstring describing what the tool does and when it should be used, this will be read by our LLM/agent and used to decide when to use the tool, and also how to use the tool.\n",
        "\n",
        "* Clear parameter names that ideally tell the LLM what each parameter is, if it isn't clear we make sure the docstring explains what the parameter is for and how to use it.\n",
        "\n",
        "* Both parameter and return type annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Raw-I6J9HyBh"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pts3fTiHyBh"
      },
      "source": [
        "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5tBy0NHyBh",
        "outputId": "49905921-7a07-40b5-c571-c1491a3dccd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x7a001f31d8a0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "add"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBnnesZHyBh"
      },
      "source": [
        "We can see the tool name, description, and arg schema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jYtOtqHyBh",
        "outputId": "1f7f3d45-e87f-46e5-e6ff-60a3395109c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add.name='add'\n",
            "add.description=\"Add 'x' and 'y'.\"\n"
          ]
        }
      ],
      "source": [
        "print(f\"{add.name=}\\n{add.description=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQS-TOjjHyBh",
        "outputId": "573c320b-c0c3-413d-8a08-41b71b3f6e9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Add 'x' and 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'add',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "add.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMqevcXOHyBi",
        "outputId": "23b60792-4de3-4151-a943-3bc8cc2b8b1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Raise 'x' to the power of 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'exponentiate',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "exponentiate.args_schema.model_json_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rI3zmLLHyBi"
      },
      "source": [
        "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4xjfsVEHyBi",
        "outputId": "ea151cb6-2587-4f5f-eff2-3b0f8625010f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 5, 'y': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
        "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
        "llm_output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgV2kgqHyBi"
      },
      "source": [
        "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXMZrD7jHyBi",
        "outputId": "60bfb8f6-5961-43d5-e0eb-8795f8d5134c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "exponentiate.func(**llm_output_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHgjrBAHyBi"
      },
      "source": [
        "This covers the basics of tools and how they work, let's move on to creating the agent itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOLTTES4HyBi"
      },
      "source": [
        "## Creating an Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PVl8e8rHyBi"
      },
      "source": [
        "We're going to construct a simple tool calling agent. We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent. We will cover LCEL more in the next chapter, but for now - all we need to know is that our agent will be constructed using syntax and components like so:\n",
        "\n",
        "\n",
        "```\n",
        "agent = (\n",
        "    <input parameters, including chat history and user query>\n",
        "    | <prompt>\n",
        "    | <LLM with tools>\n",
        ")\n",
        "```\n",
        "\n",
        "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
        "\n",
        "The agent scratchpad is where the agent will write it's _\"notes\"_ as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d3bE5bipHyBi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buOVKBS2HyBi"
      },
      "source": [
        "Next, we must define our LLM, we will use the `gpt-4o-mini` model with a `temperature` of `0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2JA_sSKeHyBi"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(temperature=0.0, model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_xvc8eHyBi"
      },
      "source": [
        "When creating an agent we need to add conversational memory to make the agent remember previous interactions. We'll be using the older `ConversationBufferMemory` class rather than the newer `RunnableWithMessageHistory` — the reason being that we will also be using the older `create_tool_calling_agent` and `AgentExecutor` method and class.\n",
        "\n",
        "In the `05` chapter we will be using the newer `RunnableWithMessageHistory` class as we'll be building a custom `AgentExecutor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvhle9X2HyBi",
        "outputId": "2635ce7d-d3e9-4ce8-afbe-21bdd74d2570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-287820297.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",  # must align with MessagesPlaceholder variable_name\n",
        "    return_messages=True  # to return Message objects\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TkhrZ5OHyBi"
      },
      "source": [
        "Now we will initialize our agent. For that we need:\n",
        "\n",
        "* `llm`: as already defined\n",
        "* `tools`: to be defined (just a list of our previously defined tools)\n",
        "* `prompt`: as already defined\n",
        "* `memory`: as already defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WOrJMHCiHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOaGuo4PHyBj"
      },
      "source": [
        "Our `agent` by itself is like one-step of our agent execution loop. So, if we call the `agent.invoke` method it will get the LLM to generate a single response and go no further, so no tools will be executed, and no next iterations will be performed.\n",
        "\n",
        "We can see this by asking a query that should trigger a tool call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqkrelxIHyBj",
        "outputId": "9ef4c30b-38fd-40f8-8f52-4745a2b4168d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ToolAgentAction(tool='multiply', tool_input={'y': 7.68, 'x': 10.7}, log=\"\\nInvoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"y\": 7.68, \"x\": 10.7}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--4850b1a1-6237-49c2-9d18-97a73a45f391-0', tool_calls=[{'name': 'multiply', 'args': {'y': 7.68, 'x': 10.7}, 'id': '54ae0f05-1dbb-4a8b-81aa-43bf0311ad63', 'type': 'tool_call'}], usage_metadata={'input_tokens': 222, 'output_tokens': 24, 'total_tokens': 337, 'input_token_details': {'cache_read': 0}})], tool_call_id='54ae0f05-1dbb-4a8b-81aa-43bf0311ad63')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "agent.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUKAebFHyBj"
      },
      "source": [
        "Here, we can see the LLM has generated that we should use the `multiply` tool and the tool input should be `{\"x\": 10.7, \"y\": 7.68}`. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc.\n",
        "\n",
        "We use the `AgentExecutor` class to handle the execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v1211mPQHyBj"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYIYOo2LHyBj"
      },
      "source": [
        "Now let's try the same query with the executor, note that the `intermediate_steps` parameter that we added before is no longer needed as the executor handles it internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnM6cRCEHyBj",
        "outputId": "3b798814-b66f-4429-e8d5-863cf08160b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m82.17599999999999\u001b[0m\u001b[32;1m\u001b[1;3m10.7 multiplied by 7.68 is 82.176.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is 10.7 multiplied by 7.68?',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': '10.7 multiplied by 7.68 is 82.176.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"chat_history\": memory.chat_memory.messages,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DztfJ_x0HyBj"
      },
      "source": [
        "We can see that the `multiply` tool was invoked, producing the observation of `82.175999...`. After the observation was provided, we can see that the LLM then generated a final response of:\n",
        "\n",
        "```\n",
        "10.7 multiplied by 7.68 is approximately 82.18.\n",
        "```\n",
        "\n",
        "This final response was generated based on the original query and the tool output (ie the _observation_). We can also confirm that this answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nJeYJSmHyBj",
        "outputId": "dfec8717-6f71-49a8-e458-aa4399a877c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82.17599999999999"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "10.7*7.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9gxtRTHyBj"
      },
      "source": [
        "Let's test our agent with some memory and tool use. First, we tell it our name, then we will perform a few tool calls, then see if the agent can still recall our name.\n",
        "\n",
        "First, give the agent our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNnFrhYwHyBk",
        "outputId": "2da1e4f9-01be-41d5-cb9b-eeedb98f32d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHello James, how can I help you?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is James',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James, how can I help you?', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'Hello James, how can I help you?'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"My name is James\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Tx9CnoHyBk"
      },
      "source": [
        "Now let's try and get the agent to perform multiple tool calls within a single execution loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tQA-L3qHyBk",
        "outputId": "20168d4b-a70b-4937-f617-523ca64243f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add` with `{'y': 10.0, 'x': 9.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m19.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'y': 2.0, 'x': 4.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m8.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `subtract` with `{'y': 19.0, 'x': 8.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m11.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `exponentiate` with `{'y': 3.0, 'x': 11.0}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m1331.0\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 1331.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James, how can I help you?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'The answer is 1331.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
        "    \"chat_history\": memory\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-v8pfp1HyBk"
      },
      "source": [
        "Let's confirm that the answer is accurate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN8RDoySHyBk",
        "outputId": "1294f893-1080-4e2b-b3a3-e6e95d8ec836"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-493"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "9+10-(4*2)**3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68FsbORDHyBk"
      },
      "source": [
        "Perfect, now let's see if the agent can still recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVJwmPxQHyBk",
        "outputId": "9c3b33c1-0a87-41f0-ef13-45fa7e5f1bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI do not know your name. I am a large language model, able to communicate in response to a wide range of prompts and questions, but I have no memory of past conversations.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name, look into memory and say',\n",
              " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='10.7 multiplied by 7.68 is 82.176.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Hello James, how can I help you?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 1331.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='I do not know your name. I am a large language model, able to communicate in response to a wide range of prompts and questions, but I have no memory of past conversations.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name, look into memory and say', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='I do not know your name. I am a large language model, able to communicate in response to a wide range of prompts and questions, but I have no memory of past conversations.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name, look into memory and say', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='I do not know your name. I am a large language model, able to communicate in response to a wide range of prompts and questions, but I have no memory of past conversations.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'I do not know your name. I am a large language model, able to communicate in response to a wide range of prompts and questions, but I have no memory of past conversations.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What is my name, look into memory and say\",\n",
        "    \"chat_history\": memory.chat_memory.messages\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfuey3P3HyBk"
      },
      "source": [
        "The agent has successfully recalled our name. Let's move on to another agent example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAVfkOWMHyBk"
      },
      "source": [
        "## SerpAPI Weather Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEP_biMHyBk"
      },
      "source": [
        "In this example, we'll be using the same agent and executor setup as before, but we'll be adding the [SerpAPI](https://serpapi.com/users/sign_in) service to allow our agent to search the web for information.\n",
        "\n",
        "To use this tool, you need an API key, with the free plan you can use up to 100 searches per month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20iwu0SQHyBk",
        "outputId": "6dfef563-15df-4fb9-e85f-be516f3dbc12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your SerpAPI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") \\\n",
        "    or getpass(\"Enter your SerpAPI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wuBcxESHyBk"
      },
      "source": [
        "Here we will load the `serpapi` tool directly from the prebuilt tools that LangChain provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YSfVtDtvHyBk"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "toolbox = load_tools(tool_names=['serpapi'], llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMwqKF61HyBk"
      },
      "source": [
        "These custom tools can look into your IP address, find out where you are currently, then we will also use a secondary function to get the current date and time, then we will use this information to feed into the SerpAPI to find us the weather pattern in your area and at the time of the function calling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nDdVlLaAHyBk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def get_location_from_ip():\n",
        "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"https://ipinfo.io/json\")\n",
        "        data = response.json()\n",
        "        if 'loc' in data:\n",
        "            latitude, longitude = data['loc'].split(',')\n",
        "            data = (\n",
        "                f\"Latitude: {latitude},\\n\"\n",
        "                f\"Longitude: {longitude},\\n\"\n",
        "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
        "                f\"Country: {data.get('country', 'N/A')}\"\n",
        "            )\n",
        "            return data\n",
        "        else:\n",
        "            return \"Location could not be determined.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Return the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Y9jMlLHyBl"
      },
      "source": [
        "We can create our prompt, this time we'll skip the `chat_history` part as we don't need it. However, you can add it if preferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LclOCUqxHyBl"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp__t8sSHyBl"
      },
      "source": [
        "Now we create our full `tools` list, our `agent`, and the `agent_executor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a7xGa_U8HyBl"
      },
      "outputs": [],
      "source": [
        "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYT36aHyBl"
      },
      "source": [
        "For me I have to specify to the AI as I live in the UK and alot of places in the UK also exist in USA, which is why I explicitly state to use the country in the search as well as the town / city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBAxuVhVHyBl",
        "outputId": "9ec9e532-ca83-4700-cfe7-40dcc2ba51a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_datetime` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m2025-07-15 12:06:28\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_location_from_ip` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mLatitude: 32.8546,\n",
            "Longitude: -79.9748,\n",
            "City: North Charleston,\n",
            "Country: US\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search` with `{'Search_query': 'weather North Charleston US in Celsius'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '26', 'unit': 'Celsius', 'precipitation': '0%', 'humidity': '89%', 'wind': '5 km/h', 'location': 'North Charleston, SC', 'date': 'Tuesday 8:00 AM', 'weather': 'Partly cloudy'}\u001b[0m\u001b[32;1m\u001b[1;3mThe current date and time is 2025-07-15 12:06:28. The weather in North Charleston, US is partly cloudy with a temperature of 26 degrees Celsius. The precipitation is 0%, humidity is 89%, and wind is 5 km/h.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "out = agent_executor.invoke({\n",
        "    \"input\": (\n",
        "        \"I have a few questions, what is the date and time right now? \"\n",
        "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
        "    )\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "DNm-X4ybHyBl",
        "outputId": "8d50a846-72c7-4a63-d30c-c9f3e63c8e74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The current date and time is 2025-07-15 12:06:28. The weather in North Charleston, US is partly cloudy with a temperature of 26 degrees Celsius. The precipitation is 0%, humidity is 89%, and wind is 5 km/h."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(out[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EYV_5l3HyBl"
      },
      "source": [
        "That's the correct answer, and we even get the approximate answer in Celsius despite the tool returning the temperature in Fahrenheit.\n",
        "\n",
        "We've finished our into to LangChain Agents, in the next chapter we will be looking at how to create custom agents and executors.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}