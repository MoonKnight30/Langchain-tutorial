{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsMFKCALr3y"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQji83qLpVC"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dLBWVZLpVD"
      },
      "source": [
        "# LangChains Expression Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRUqmfBLpVE"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i690tIabLwb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abce3536-8703-4b0f-ff15-3f356b8074e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/441.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.6/441.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.9/367.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core \\\n",
        "  langchain-google-genai \\\n",
        "  langchain-community \\\n",
        "  langsmith \\\n",
        "  google-generativeai \\\n",
        "  docarray==0.40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOR87lEpLpVE"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfpD-R6sLpVE"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "exBqQHgqLpVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef300c27-dd36-4220-af0c-b9f6eb282742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n",
            "Enter Google API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# LangSmith Setup (optional, for observability)\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-prompts-gemini\"\n",
        "\n",
        "# Google Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or \\\n",
        "    getpass(\"Enter Google API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGeByKjLpVF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQbgErgdLpVF"
      },
      "source": [
        "## Traditional Chains vs LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyluns_ELpVF"
      },
      "source": [
        "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTZTTiK7LpVF"
      },
      "source": [
        "### Traditional LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHfbKZ7LpVF"
      },
      "source": [
        "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
        "\n",
        "Let's see how we construct this using the traditional method, for this we need:\n",
        "\n",
        "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
        "* `llm` — the LLM we will be using to generate the output.\n",
        "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jyz2vWLoLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"Give me a small report on {topic}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rykro39YLpVF"
      },
      "source": [
        "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uvs7hILRLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(temperature=0.0, model=\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhs6_3c4LpVF",
        "outputId": "9d592171-00dd-40d5-9bb0-8cdf95ae58dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--1c87a0fe-9ec1-4a79-80dd-ea916aa01375-0', usage_metadata={'input_tokens': 3, 'output_tokens': 10, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39qjMpLLpVG"
      },
      "source": [
        "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "46qTjklnLpVG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z0uwxedBLpVG",
        "outputId": "4dee1c56-2d86-4ef4-d13b-97615ef56bcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello there! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "out = output_parser.invoke(llm_out)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4CUy52LpVG"
      },
      "source": [
        "Through the `LLMChain` class we can place each of our components into a linear `chain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PrFk1J9dLpVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0aa503-4c0c-4423-dc25-62f8a4a148ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNsVMVpLpVG"
      },
      "source": [
        "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
        "\n",
        "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDDq8Cq7LpVG",
        "outputId": "0488a0ed-6edd-4b6f-9d7b-94a92f578415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'retrieval augmented generation',\n",
              " 'text': '## Small Report: Retrieval Augmented Generation (RAG)\\n\\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\\n\\n**Introduction:**\\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\\n\\n**How it Works:**\\nRAG operates in three primary steps:\\n\\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\\n\\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user\\'s question and the relevant context retrieved from the external source.\\n\\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\\n\\n**Key Benefits:**\\n\\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\\n*   **Access to Up-to-Date Information:** Bypasses the LLM\\'s training data cutoff, allowing it to incorporate real-time or frequently updated data.\\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn\\'t part of their original training.\\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\\n\\n**Challenges & Considerations:**\\n\\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\\n\\n**Applications:**\\nRAG is being widely adopted across various sectors, including:\\n\\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\\n*   **Education:** Creating personalized learning experiences with up-to-date content.\\n\\n**Conclusion:**\\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions.'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbHqc1qLLpVG"
      },
      "source": [
        "We can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "hBxPHOZ-LpVG",
        "outputId": "90dad0d8-50e9-4c25-dc9c-adaeca6a7aa1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Small Report: Retrieval Augmented Generation (RAG)\n\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\n\n**Introduction:**\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\n\n**How it Works:**\nRAG operates in three primary steps:\n\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\n\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user's question and the relevant context retrieved from the external source.\n\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\n\n**Key Benefits:**\n\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\n*   **Access to Up-to-Date Information:** Bypasses the LLM's training data cutoff, allowing it to incorporate real-time or frequently updated data.\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn't part of their original training.\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\n\n**Challenges & Considerations:**\n\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\n\n**Applications:**\nRAG is being widely adopted across various sectors, including:\n\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\n*   **Education:** Creating personalized learning experiences with up-to-date content.\n\n**Conclusion:**\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbmq0DfeLpVG"
      },
      "source": [
        "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWets8OpLpVG"
      },
      "source": [
        "## LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lr5NA0xLpVG"
      },
      "source": [
        "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8R-9b7ulLpVH"
      },
      "outputs": [],
      "source": [
        "lcel_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJvzetzLpVH"
      },
      "source": [
        "We can `invoke` this chain in the same way as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "XKWzIwpCLpVH",
        "outputId": "e68d4d47-fec1-4b9a-86dc-569b2b16370f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'## Small Report: Retrieval Augmented Generation (RAG)\\n\\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\\n\\n**Introduction:**\\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\\n\\n**How it Works:**\\nRAG operates in three primary steps:\\n\\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\\n\\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user\\'s question and the relevant context retrieved from the external source.\\n\\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\\n\\n**Key Benefits:**\\n\\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\\n*   **Access to Up-to-Date Information:** Bypasses the LLM\\'s training data cutoff, allowing it to incorporate real-time or frequently updated data.\\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn\\'t part of their original training.\\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\\n\\n**Challenges & Considerations:**\\n\\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\\n\\n**Applications:**\\nRAG is being widely adopted across various sectors, including:\\n\\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\\n*   **Education:** Creating personalized learning experiences with up-to-date content.\\n\\n**Conclusion:**\\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7E8_msHLpVH"
      },
      "source": [
        "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "42uPzB1JLpVH",
        "outputId": "e9cd4c55-1de4-4710-bef1-ca0ff1c91e5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Small Report: Retrieval Augmented Generation (RAG)\n\n**Title:** Retrieval Augmented Generation (RAG): Enhancing LLM Accuracy and Relevance\n\n**Introduction:**\nRetrieval Augmented Generation (RAG) is a paradigm-shifting technique designed to enhance the capabilities of Large Language Models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), outdated knowledge, or a lack of specific context for niche queries. RAG addresses these limitations by combining the generative power of LLMs with the precision of information retrieval systems.\n\n**How it Works:**\nRAG operates in three primary steps:\n\n1.  **Retrieval:** When a user poses a query, the RAG system first searches a vast external knowledge base (e.g., a database of documents, articles, internal company data, or the internet). It identifies and retrieves the most relevant pieces of information, often called \"chunks\" or \"documents,\" that are pertinent to the query. This knowledge base can be continuously updated, ensuring the information is current.\n\n2.  **Augmentation:** The retrieved information is then used to \"augment\" or enrich the original user query. Instead of just sending the raw query to the LLM, the system constructs a new, more comprehensive prompt that includes both the user's question and the relevant context retrieved from the external source.\n\n3.  **Generation:** Finally, this augmented prompt is fed into the LLM. The LLM then generates a response, but critically, it does so by grounding its answer in the provided retrieved context. This significantly reduces the likelihood of hallucinations and ensures the response is factual, relevant, and specific to the information found.\n\n**Key Benefits:**\n\n*   **Enhanced Accuracy & Factuality:** Reduces the risk of LLMs generating incorrect or fabricated information by providing verifiable sources.\n*   **Access to Up-to-Date Information:** Bypasses the LLM's training data cutoff, allowing it to incorporate real-time or frequently updated data.\n*   **Domain Specificity:** Enables LLMs to answer questions about proprietary, internal, or highly specialized knowledge that wasn't part of their original training.\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining (fine-tuning) of LLMs every time new information becomes available.\n*   **Transparency & Attribution:** Can often provide citations or links to the source documents from which the information was retrieved, increasing user trust.\n*   **Improved Explainability:** Makes it easier to understand *why* an LLM generated a particular answer, as the source context is explicit.\n\n**Challenges & Considerations:**\n\n*   **Retrieval Quality:** The effectiveness of RAG heavily depends on the quality of the retrieval step. Poorly retrieved information will lead to poor generation (\"garbage in, garbage out\").\n*   **Context Window Limits:** LLMs have limits on how much text they can process in a single prompt, which can restrict the amount of retrieved context.\n*   **Complexity:** Building and maintaining a robust RAG system, including data indexing, chunking strategies, and retrieval algorithms, can be complex.\n*   **Latency:** The retrieval step adds an extra processing layer, potentially increasing response times compared to a standalone LLM.\n\n**Applications:**\nRAG is being widely adopted across various sectors, including:\n\n*   **Customer Support:** Providing accurate answers from extensive knowledge bases.\n*   **Enterprise Search:** Enabling employees to quickly find specific information within internal documents.\n*   **Healthcare:** Answering medical queries based on the latest research and patient records.\n*   **Legal:** Summarizing case law and retrieving relevant statutes.\n*   **Education:** Creating personalized learning experiences with up-to-date content.\n\n**Conclusion:**\nRetrieval Augmented Generation represents a significant leap forward in making LLMs more reliable, trustworthy, and practical for real-world applications. By intelligently combining the strengths of information retrieval with the generative power of large language models, RAG mitigates key limitations of standalone LLMs, paving the way for more accurate, contextually relevant, and verifiable AI-driven solutions."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbutFcpyLpVI"
      },
      "source": [
        "### How Does the Pipe Operator Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjOY7VILpVI"
      },
      "source": [
        "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
        "\n",
        "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
        "\n",
        "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
        "\n",
        "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9xTvwvF4LpVI"
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            return other.invoke(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "    def invoke(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Vg_PhXLpVI"
      },
      "source": [
        "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
        "\n",
        "First, let's create a few functions that we'll chain together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G7HCw9-VLpVI"
      },
      "outputs": [],
      "source": [
        "def add_five(x):\n",
        "    return x+5\n",
        "\n",
        "def sub_five(x):\n",
        "    return x-5\n",
        "\n",
        "def mul_five(x):\n",
        "    return x*5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDvmtWLpVI"
      },
      "source": [
        "Now we wrap our functions with the `Runnable`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LfnBBcbxLpVI"
      },
      "outputs": [],
      "source": [
        "add_five_runnable = Runnable(add_five)\n",
        "sub_five_runnable = Runnable(sub_five)\n",
        "mul_five_runnable = Runnable(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHLbl_7LpVI"
      },
      "source": [
        "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHFo61TwLpVI",
        "outputId": "2bdbde36-a8f8-40f1-a0b8-ca7422711f2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oim1zWHLpVI"
      },
      "source": [
        "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh3-jF8pLpVJ",
        "outputId": "ae123783-9176-404c-e226-44fcbf6a962d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lnj00T8LpVJ"
      },
      "source": [
        "## LCEL `RunnableLambda`\n",
        "\n",
        "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BmMH8GzVLpVJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_five_runnable = RunnableLambda(add_five)\n",
        "sub_five_runnable = RunnableLambda(sub_five)\n",
        "mul_five_runnable = RunnableLambda(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkpwIKq6LpVJ"
      },
      "source": [
        "We chain these together again with the pipe `|` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9e58vaSELpVJ"
      },
      "outputs": [],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FdqbdAyLpVJ"
      },
      "source": [
        "And call them using the `invoke` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHBMT8ULpVJ",
        "outputId": "e46cb785-763c-4603-da9a-868f611023ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47mu4xxqLpVJ"
      },
      "source": [
        "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xRJLzshqLpVJ"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"give me a small report about {topic}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lfl_s4VALpVJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "WSMlRM8wLpVJ",
        "outputId": "75a81363-7f4f-442e-a17c-1717bce1200b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Artificial Intelligence (AI): A Concise Report\n\n**Introduction**\nArtificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI is no longer a futuristic concept but a rapidly evolving field that is transforming industries and daily life.\n\n**What is AI?**\nAt its core, AI involves developing algorithms that enable computers to analyze vast amounts of data, identify patterns, and make decisions or predictions. Key sub-fields include:\n*   **Machine Learning (ML):** Systems learn from data without explicit programming.\n*   **Deep Learning (DL):** A subset of ML that uses neural networks with many layers to learn complex patterns, often used for image and speech recognition.\n*   **Natural Language Processing (NLP):** Enables computers to understand, interpret, and generate human language.\n*   **Computer Vision:** Allows machines to \"see\" and interpret visual information from the world.\n\n**Key Applications & Impact**\nAI's influence is pervasive, impacting various sectors:\n*   **Healthcare:** Drug discovery, personalized treatment plans, diagnostic tools, and robotic surgery.\n*   **Finance:** Fraud detection, algorithmic trading, and personalized financial advice.\n*   **Transportation:** Self-driving cars, traffic optimization, and logistics.\n*   **Customer Service:** Chatbots and virtual assistants providing instant support.\n*   **Entertainment:** Content recommendation systems (e.g., Netflix, Spotify), and generative AI for art and music.\n*   **Manufacturing:** Predictive maintenance, quality control, and robotic automation.\n\n**Benefits of AI**\n*   **Increased Efficiency & Automation:** Automating repetitive tasks, freeing up human resources for more complex work.\n*   **Enhanced Decision-Making:** Analyzing large datasets to provide insights and support better, faster decisions.\n*   **Innovation & Problem Solving:** Enabling breakthroughs in scientific research and addressing complex global challenges.\n*   **Personalization:** Delivering tailored experiences in various services, from shopping to education.\n\n**Challenges & Considerations**\nWhile promising, AI development faces significant challenges:\n*   **Ethical Concerns:** Issues of bias in algorithms, privacy of data, and accountability for AI decisions.\n*   **Job Displacement:** Potential for automation to displace human jobs in certain sectors.\n*   **Complexity & Control:** Ensuring AI systems remain controllable and aligned with human values as they become more sophisticated.\n*   **Security:** Protecting AI systems from malicious attacks and misuse.\n\n**Conclusion**\nArtificial Intelligence is a powerful and rapidly evolving technology that is reshaping our world. Its continued development promises further advancements across all aspects of society, offering immense potential for progress and innovation. However, realizing this potential responsibly requires careful consideration of its ethical, social, and economic implications to ensure a beneficial and equitable future for all."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7HOxwaLpVJ"
      },
      "source": [
        "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wc7ws8FkLpVJ"
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "old_word = \"AI\"\n",
        "new_word = \"skynet\"\n",
        "\n",
        "def replace_word(x):\n",
        "    return x.replace(old_word, new_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHrg5iSLpVK"
      },
      "source": [
        "Lets wrap these functions and see what the output is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_vFeqMY-LpVK"
      },
      "outputs": [],
      "source": [
        "extract_fact_runnable = RunnableLambda(extract_fact)\n",
        "replace_word_runnable = RunnableLambda(replace_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TDWN-tQzLpVK"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "OCODP8vLLpVK",
        "outputId": "64bf9ffe-513b-491b-cf09-c68e270df5dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Introduction**\nRetrieval Augmented Generation (RAG) is a technique designed to enhance the capabilities of large language models (LLMs) by providing them with access to external, up-to-date, and domain-specific information. While LLMs are powerful in generating human-like text, they often suffer from \"hallucinations\" (generating factually incorrect information), are limited by their training data cutoff, and lack specific knowledge about proprietary or niche subjects. RAG addresses these limitations by grounding the LLM's responses in verifiable, external data.\n**How it Works**\nRAG operates in three primary phases:\n1.  **Retrieval Phase:**\n    *   When a user poses a query, the system first analyzes it to understand the user's intent and keywords.\n    *   It then searches a vast external knowledge base (e.g., a database of documents, articles, web pages, or a vector database containing embeddings of these sources) to find relevant information snippets or documents. This knowledge base is separate from the LLM's core training data.\n2.  **Augmentation Phase:**\n    *   The retrieved relevant information is then combined with the original user query.\n    *   This combined input (query + retrieved context) forms an \"augmented prompt\" that is much richer and more specific than the original query alone.\n3.  **Generation Phase:**\n    *   The augmented prompt is fed into the LLM.\n    *   The LLM then uses this combined information to formulate a more accurate, relevant, and grounded response, drawing directly from the provided context rather than solely relying on its internal, potentially outdated, or generic knowledge.\n**Key Benefits**\n*   **Improved Accuracy and Factuality:** Significantly reduces hallucinations by providing the LLM with verifiable facts.\n*   **Access to Up-to-Date Information:** Allows LLMs to answer questions about recent events or newly published data, bypassing their training data cutoff.\n*   **Domain-Specific Knowledge:** Enables LLMs to provide expert answers on proprietary or niche topics by connecting them to private knowledge bases (e.g., company documents, medical records, legal texts).\n*   **Transparency and Explainability:** In many RAG implementations, the system can cite the sources from which it retrieved information, allowing users to verify the facts.\n*   **Reduced Training Costs:** Eliminates the need for expensive and time-consuming retraining or fine-tuning of LLMs every time new information becomes available.\n**Applications**\nRAG is being widely adopted across various sectors, including:\n*   **Customer Support Chatbots:** Providing accurate and up-to-date answers to customer queries based on product manuals, FAQs, and service policies.\n*   **Knowledge Management:** Enabling employees to quickly find specific information within vast internal company documents.\n*   **Research Assistance:** Helping researchers summarize papers, find specific data points, or answer complex questions by querying academic databases.\n*   **Healthcare and Legal:** Assisting professionals in retrieving relevant case law, medical guidelines, or patient information.\n*   **Content Creation:** Generating more factual and well-researched articles, reports, or summaries.\n**Conclusion**\nRetrieval Augmented Generation represents a significant step forward in making LLMs more reliable, versatile, and practical for real-world applications. By bridging the gap between the vast generative power of LLMs and the critical need for factual accuracy and real-time information, RAG is becoming a standard practice for deploying robust and trustworthy skynet-powered solutions."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSumR1tLpVK"
      },
      "source": [
        "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXKLyQKLpVK"
      },
      "source": [
        "## LCEL `RunnableParallel` and `RunnablePassthrough`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avFhxg_pLpVK"
      },
      "source": [
        "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
        "\n",
        "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
        "\n",
        "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NOruHLadzRY",
        "outputId": "e7ef3eed-095e-40af-cfa4-2bf019143ef6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m752.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTsqIAufLpVK"
      },
      "source": [
        "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaD678FMLpVK"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# Use a lightweight and performant model\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"half the info is here\",\n",
        "        \"DeepSeek-V3 was released in December 2024\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")\n",
        "\n",
        "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"the other half of the info is here\",\n",
        "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub4pjRrBLpVK"
      },
      "source": [
        "Here you can see the prompt does have three inputs, two for context and one for the question itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pnxXrw-CLpVK"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
        "Context:\n",
        "{context_a}\n",
        "{context_b}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jL3YN1c1LpVK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YchbLoKLpVK"
      },
      "source": [
        "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xkhFV8-SLpVL"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "retriever_a = vecstore_a.as_retriever()\n",
        "retriever_b = vecstore_b.as_retriever()\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtORU5xLpVL"
      },
      "source": [
        "The chain we'll be constructing will look something like this:\n",
        "\n",
        "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PNLpfS3_LpVL"
      },
      "outputs": [],
      "source": [
        "chain = retrieval | prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8BfprxLpVL"
      },
      "source": [
        "We `invoke` it as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "84gEXUG1LpVL",
        "outputId": "39a2d489-7771-4497-c3ce-2ed1e7e2bc86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The DeepSeek-V3 LLM, released in December, uses a mixture of experts architecture.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "result = chain.invoke(\n",
        "    \"what architecture does the model DeepSeek released in december use?\"\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwr16jsLpVL"
      },
      "source": [
        "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}